package com.iai.flink;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.FilterFunction;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.sink.SinkFunction;
import org.apache.http.client.methods.HttpPost;
import org.apache.http.entity.StringEntity;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClients;

/**
 * å®æ—¶å¼‚å¸¸æ£€æµ‹ Flink ä»»åŠ¡
 * ä» Kafka è¯»å–ä¼ æ„Ÿå™¨æ•°æ®ï¼Œæ£€æµ‹åˆ° ANOMALY çŠ¶æ€æ—¶ï¼Œè°ƒç”¨ AgentServer çš„ Webhookã€‚
 */
public class AnomalyDetectionJob {

    public static void main(String[] args) throws Exception {
        // 1. è®¾ç½®æ‰§è¡Œç¯å¢ƒ
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 2. é…ç½® Kafka Source
        KafkaSource<String> source = KafkaSource.<String>builder()
                .setBootstrapServers("192.168.0.105:9092")
                .setTopics("raw_sensor_data")
                .setGroupId("flink-group")
                .setStartingOffsets(OffsetsInitializer.latest())
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .build();

        // 3. è¯»å–æµ
        DataStream<String> stream = env.fromSource(source, WatermarkStrategy.noWatermarks(), "Kafka Source");

        // 4. å®æ—¶æ£€æµ‹é€»è¾‘ï¼šè¿‡æ»¤å‡ºçŠ¶æ€ä¸º ANOMALY çš„æ•°æ®
        DataStream<String> anomalyStream = stream.filter(new FilterFunction<String>() {
            private final ObjectMapper mapper = new ObjectMapper();
            @Override
            public boolean filter(String value) throws Exception {
                JsonNode node = mapper.readTree(value);
                // æ£€æŸ¥ status å­—æ®µæ˜¯å¦ä¸º ANOMALY
                return node.has("status") && "ANOMALY".equals(node.get("status").asText());
            }
        });

        // 5. è°ƒç”¨ AgentServer Webhook (è‡ªå®šä¹‰ Sink)
        anomalyStream.addSink(new HttpWebhookSink());

        System.out.println("âš¡ Flink å®æ—¶æ£€æµ‹ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨ç›‘å¬ Kafka: raw_sensor_data...");
        env.execute("IIoT Anomaly Detection Job");
    }

    /**
     * è‡ªå®šä¹‰ Sinkï¼šå°†å¼‚å¸¸æ•°æ®å‘é€è‡³ AgentServer Webhook
     */
    public static class HttpWebhookSink implements SinkFunction<String> {
        private final String webhookUrl = "http://127.0.0.1:8000/api/v1/alerts";

        @Override
        public void invoke(String value, Context context) throws Exception {
            System.out.println("ğŸš¨ Flink [å‘ç°å¼‚å¸¸æ•°æ®] -> æ­£åœ¨è¯·æ±‚ AgentServer: " + value);

            try (CloseableHttpClient httpClient = HttpClients.createDefault()) {
                HttpPost httpPost = new HttpPost(webhookUrl);
                StringEntity entity = new StringEntity(value, "UTF-8");
                httpPost.setEntity(entity);
                httpPost.setHeader("Content-type", "application/json");

                httpClient.execute(httpPost);
                System.out.println("âœ… å·²ä¸‹å‘æ™ºèƒ½è¯Šæ–­ä»»åŠ¡ã€‚");
            } catch (Exception e) {
                System.err.println("âŒ è°ƒç”¨ AgentServer å¤±è´¥: " + e.getMessage());
            }
        }
    }
}
