import os
import sys
import uuid
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Request, BackgroundTasks
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from openai import AsyncOpenAI

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from logger import logger, get_logger_with_trace
from agents.diagnostic_agent import DiagnosticAgent
from agents.decision_agent import DecisionAgent
from services.alert_service import AlertService
from services.nacos_registry import register_to_nacos, deregister_from_nacos, start_heartbeat, stop_heartbeat

# === åˆå§‹åŒ–ç¯å¢ƒä¸å®¢æˆ·ç«¯ ===
# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o")

# åˆå§‹åŒ–çœŸå®çš„ LLM å®¢æˆ·ç«¯
llm_client = AsyncOpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# å­˜å‚¨å…¨å±€çš„ MCP ClientSession å’Œå·¥å…·ç¼“å­˜
class AppState:
    mcp_session: Optional[ClientSession] = None
    tools_cache: List[Dict[str, Any]] = []

state = AppState()

# === å¸®åŠ©å‡½æ•°ï¼šå°† MCP Schema è½¬ä¸º OpenAI Tools Schema ===
def mcp_tools_to_openai_tools(mcp_tools) -> List[Dict[str, Any]]:
    openai_tools = []
    for tool in mcp_tools:
        schema = tool.inputSchema
        openai_tools.append({
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": schema
            }
        })
    return openai_tools

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    FastAPI ç”Ÿå‘½å‘¨æœŸç®¡ç†
    åœ¨æ­¤å¤„å¯åŠ¨å¹¶è¿æ¥åˆ° MCP Server (mcp_server.py)ï¼Œå¹¶è·å–å·¥å…·åˆ—è¡¨
    """
    logger.info("ğŸš€ æ­£åœ¨å¯åŠ¨ Agent Server ç”Ÿå‘½å‘¨æœŸ...")
    from contextlib import AsyncExitStack
    
    server_params = StdioServerParameters(
        command=sys.executable,
        args=["mcp_server.py"]
    )
    
    async with AsyncExitStack() as stack:
        try:
            stdio_transport = await stack.enter_async_context(stdio_client(server_params))
            read, write = stdio_transport
            
            session = await stack.enter_async_context(ClientSession(read, write))
            await session.initialize()
            
            state.mcp_session = session
            logger.info("âœ… æœ¬åœ° MCP Server (æŠ€èƒ½åº“) è¿æ¥å¹¶åˆå§‹åŒ–å®Œæˆï¼")
            
            # æ‹‰å–å®Œæ•´çš„å·¥å…·æ ˆå¹¶ç¼“å­˜ä¸º OpenAI æ ¼å¼
            tools_response = await session.list_tools()
            state.tools_cache = mcp_tools_to_openai_tools(tools_response.tools)
            logger.info(f"âœ… æˆåŠŸåŠ è½½å·¥å…·: {[t.name for t in tools_response.tools]}")
            
            # [Phase E] Nacos æœåŠ¡æ³¨å†Œ
            register_to_nacos()
            start_heartbeat()
            
            yield
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨ MCP Client å¤±è´¥: {e}", exc_info=True)
            raise e
        finally:
            stop_heartbeat()
            deregister_from_nacos()
            logger.info("ğŸ›‘ æ­£åœ¨å…³é—­æœåŠ¡å’Œé‡Šæ”¾èµ„æº...")

app = FastAPI(
    title="IIoT Expert Agent API",
    description="å·¥ä¸šç‰©è”ç½‘è®¾å¤‡è¯Šæ–­å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
    version="1.0.0",
    lifespan=lifespan
)

# === æ³¨å†Œ Web ç®¡ç†åå°è·¯ç”± ===
from routes.dashboard import router as dashboard_router
app.include_router(dashboard_router)

# === æ•°æ®æ¨¡å‹å®šä¹‰ ===
class AlertPayload(BaseModel):
    device_id: str = Field(..., description="è®¾å¤‡å”¯ä¸€æ ‡è¯†ç¬¦")
    status: str = Field(..., description="è®¾å¤‡çŠ¶æ€ (å¦‚ NORMAL, ANOMALY)")
    temperature: float = Field(..., description="å®æ—¶æ¸©åº¦")
    vibration: float = Field(..., description="å®æ—¶éœ‡åŠ¨")
    timestamp: str = Field(..., description="å‘Šè­¦å‘ç”Ÿæ—¶é—´")

class ChatRequest(BaseModel):
    query: str = Field(..., description="ç”¨æˆ·çš„æé—®å†…å®¹")
    session_id: Optional[str] = Field(default=None, description="å¯¹è¯ Session æ ‡è¯†")

class DiagnosisResponse(BaseModel):
    trace_id: str
    message: str

class ChatResponse(BaseModel):
    trace_id: str
    answer: str

# === å¤šæ™ºèƒ½ä½“ååŒä½œæˆ˜å…¥å£ (Agent Router) ===
async def process_alert_task(trace_id: str, alert_data: dict):
    """
    èƒŒæ™¯ä»»åŠ¡ï¼šæ¥æ”¶åˆ°å‘Šè­¦åçš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµã€‚
    """
    req_logger = get_logger_with_trace(trace_id)
    device_id = alert_data.get("device_id", "UNKNOWN")
    
    # [Phase 2] 0. å‘Šè­¦å»é‡åˆ¤æ–­ (Redis 5åˆ†é’Ÿå†·å´çª—)
    if AlertService.is_cooling_down(device_id):
        req_logger.warning(f"â„ï¸ è®¾å¤‡ {device_id} åœ¨5åˆ†é’Ÿå†·å´æœŸå†…ï¼Œæœ¬æ¬¡é‡å¤å‘Šè­¦è¢«è¿‡æ»¤ã€‚")
        return
        
    # [Phase 2] 0.5 è®°å½•å‘Šè­¦å…¥åº“
    req_logger.info(f"ğŸ“ å¼€å§‹å¤„ç†è®¾å¤‡å‘Šè­¦, Data: {alert_data}")
    AlertService.record_alert(trace_id, alert_data)
    
    try:
        # 1. å¯åŠ¨è¯Šæ–­ä¸“å®¶
        req_logger.info("ğŸ‘¨â€âš•ï¸ [æ­¥éª¤ 1] å¯åŠ¨è¯Šæ–­ä¸“å®¶ (Diagnostic Expert)...")
        from services.event_bus import event_bus
        event_bus.publish("global_stream", f"<b>[{trace_id[:8]}]</b> ğŸ‘¨â€âš•ï¸ å¯åŠ¨è¯Šæ–­ä¸“å®¶å¯¹ {device_id} è¿›è¡Œæ’æŸ¥...")
        diagnostic_agent = DiagnosticAgent(
            llm_client=llm_client, 
            mcp_session=state.mcp_session, 
            model_name=MODEL_NAME,
            trace_id=trace_id
        )
        report = await diagnostic_agent.diagnose(alert_data, tools=state.tools_cache)
        req_logger.info(f"ğŸ“„ è¯Šæ–­æŠ¥å‘Šå‡ºç‚‰:\n{report}")
        event_bus.publish("global_stream", f"<b>[{trace_id[:8]}]</b> ğŸ“„ è¯Šæ–­æŠ¥å‘Šå·²ç”Ÿæˆ")
        
        # 2. å¯åŠ¨å†³ç­–ä¸“å®¶
        req_logger.info("ğŸ‘¨â€âš–ï¸ [æ­¥éª¤ 2] å¯åŠ¨å†³ç­–ä¸“å®¶ (Decision Maker)...")
        event_bus.publish("global_stream", f"<b>[{trace_id[:8]}]</b> ğŸ‘¨â€âš–ï¸ å¯åŠ¨å†³ç­–ä¸“å®¶æ­£åœ¨åˆ¶å®šæ–¹æ¡ˆ...")
        decision_agent = DecisionAgent(
            llm_client=llm_client,
            model_name=MODEL_NAME,
            trace_id=trace_id
        )
        decision = await decision_agent.make_decision(diagnostic_report=report)
        req_logger.info(f"ğŸ“œ æœ€ç»ˆç»´ä¿å†³ç­–:\n{decision}")
        
        # [Phase 2] 3. è¯Šæ–­æŠ¥å‘Šå’Œå·¥å•è‡ªåŠ¨å…¥åº“è½ç›˜ (é—­ç¯)
        req_logger.info("ğŸ’¾ [æ­¥éª¤ 3] æ­£åœ¨è½ç›˜è¯Šæ–­æŠ¥å‘Šä¸å·¥å•è®°å½•...")
        AlertService.save_diagnosis(trace_id, device_id, report, decision)
        AlertService.create_work_order(trace_id, device_id, decision)
        
        req_logger.info("âœ… å‘Šè­¦æ™ºèƒ½è¯Šæ–­æµè½¬å¤„ç†å’Œå·¥å•å…¥åº“é—­ç¯å…¨éƒ¨å®Œæˆï¼")
        event_bus.publish("global_stream", f"<b>[{trace_id[:8]}]</b> âœ… æµè½¬é—­ç¯å®Œæˆï¼Œè¯Šæ–­æŠ¥å‘Šä¸å·¥å•å·²è½ç›˜ï¼")
        
    except Exception as e:
        req_logger.error(f"âŒ å¤„ç†æµè½¬å¼‚å¸¸: {e}", exc_info=True)
        from services.event_bus import event_bus
        event_bus.publish("global_stream", f"<b>[{trace_id[:8]}]</b> âŒ æ™ºèƒ½ä½“ç³»ç»Ÿå¼‚å¸¸: {e}")

# === API è·¯ç”±å®šä¹‰ ===

@app.post("/api/v1/alerts", response_model=DiagnosisResponse, summary="æ¥æ”¶å®æ—¶å‘Šè­¦")
async def receive_alert(payload: AlertPayload, background_tasks: BackgroundTasks):
    """
    æ¥æ”¶æ¥è‡ªå®æ—¶ç³»ç»Ÿ (å¦‚ Flink) çš„è®¾å¤‡å¼‚å¸¸å‘Šè­¦Webhookã€‚
    æ¥æ”¶åç«‹å³è¿”å› ACKï¼Œå¹¶åœ¨åå°å¯åŠ¨æ™ºèƒ½è¯Šæ–­å·¥ä½œæµã€‚
    """
    trace_id = uuid.uuid4().hex
    req_logger = get_logger_with_trace(trace_id)
    
    if not OPENAI_API_KEY:
        raise HTTPException(status_code=500, detail="æœªé…ç½®å¤§æ¨¡å‹ API KEY")
        
    req_logger.info(f"ğŸ“¥ æ¥æ”¶åˆ°å‘Šè­¦ payload: {payload.model_dump()}")
    
    # å°†å¤„ç†è¿‡ç¨‹æ”¾å…¥åå°ä»»åŠ¡ï¼Œå¿«é€Ÿå“åº” Flink (é¿å…è¶…æ—¶)
    background_tasks.add_task(process_alert_task, trace_id, payload.model_dump())
    
    return DiagnosisResponse(
        trace_id=trace_id,
        message="å‘Šè­¦å·²æ¥æ”¶ï¼Œè¯Šæ–­ç»„å·²å—å‘½å…¥åœºè°ƒæŸ¥ã€‚"
    )

@app.post("/api/v1/chat", response_model=ChatResponse, summary="ç»ˆç«¯èŠå¤©å…¥å£")
async def chat_endpoint(request: ChatRequest):
    """
    é¢„ç•™çš„äººæœºäº¤äº’æ¥å£ï¼Œå¯ä»¥å®ç°è‡ªç”±æé—®ç­‰èƒ½åŠ›ã€‚
    ï¼ˆæ³¨ï¼šç›®å‰ç®€å•èµ·è§ï¼Œå•æ™ºèƒ½ä½“ç›´æ¥å›å¤ï¼Œå¦‚æœéœ€è¦åŒæ ·æ”¯æŒè°ƒç”¨å·¥å…·ï¼Œå¯ä»¥å•ç‹¬å®ä¾‹åŒ–ä¸€ä¸ª Agentï¼‰
    """
    trace_id = uuid.uuid4().hex
    req_logger = get_logger_with_trace(trace_id)
    req_logger.info(f"ğŸ’¬ æ”¶åˆ°èŠå¤©è¯·æ±‚: {request.query}")
    
    # ä¸´æ—¶å¯åŠ¨ä¸€ä¸ªé€šç”¨è¯¢é—®ï¼Œæš‚ä¸è°ƒç”¨å®Œæ•´æµè½¬
    try:
        response = await llm_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å·¥å‚çš„AIåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": request.query}
            ]
        )
        answer = response.choices[0].message.content
        return ChatResponse(trace_id=trace_id, answer=answer)
    except Exception as e:
        req_logger.error(f"LLM è¯·æ±‚å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/health", summary="å¥åº·æ¢é’ˆ")
async def health_check():
    mcp_status = "healthy" if state.mcp_session is not None else "unhealthy"
    return {
        "status": "ok", 
        "mcp_connection": mcp_status, 
        "model": MODEL_NAME,
        "tools_loaded": len(state.tools_cache)
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("api:app", host="0.0.0.0", port=8000, reload=True)
